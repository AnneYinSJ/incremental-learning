import torch
import pickle
import itertools
import torch
import torch.nn as nn
import utils.utils as ut
import torch.optim as optim
import torch.utils.data as td
import trainer.classifierTrainer as t
import trainer.classifierFactory as tF
from torch.autograd import Variable

class trainer():
    def __init__(self, args, dataset, classifierTrainer, model, trainIterator,
                 testIterator, modelFactory, experiment):
        self.args = args
        self.dataset = dataset
        self.classifierTrainer = classifierTrainer
        self.model = model
        self.trainIterator = trainIterator
        self.testIterator = testIterator
        self.modelFactory = modelFactory
        self.experiment = experiment

    def train(self):
        x = []
        y = []

        for classGroup in range(0, self.dataset.classes, self.args.step_size):
            self.classifierTrainer.setupTraining()
            self.classifierTrainer.incrementClasses(classGroup)
            dataIteratorForGan = self.classifierTrainer.getTrainedDataIterator()

            epoch = 0
            for epoch in range(0, self.args.epochs_class):
                self.classifierTrainer.updateLR(epoch)
                self.classifierTrainer.train()
                if epoch % self.args.log_interval == 0:
                    print("Train Classifier",
                          self.classifierTrainer.evaluate(self.trainIterator))
                    print("Test Classifier",
                          self.classifierTrainer.evaluate(self.testIterator))

            #Get a new Generator and Discriminator
            #TODO What if we kept the Discriminator?
            G, D = self.modelFactory.getModel("cdcgan", self.args.dataset)
            if self.args.cuda:
                G.cuda()
                D.cuda()
            self.trainGan(G, D, dataIteratorForGan)

            # Saving confusion matrix
            ut.saveConfusionMatrix(int(classGroup / self.args.step_size) *
                                   self.args.epochs_class + epoch,
                                   self.experiment.path + "CONFUSION",
                                   self.model, self.args, self.dataset,
                                   self.testIterator)

            y.append(self.classifierTrainer.evaluate(self.testIterator))
            x.append(classGroup + self.args.step_size)
            ut.plotAccuracy(self.experiment, x,
                            [("Trained Classifier",y)],
                            self.dataset.classes + 1, self.args.name)

    def trainGan(self, G, D, dataIterator):
        activeClasses = self.trainIterator.dataset.activeClasses
        #TODO Change batchsize of dataIterator here to gan_batch_size
        #dataIterator.batch_size = self.args.gan_batch_size <- Doesnt work :(
        criterion = nn.BCELoss()
        G_Opt = optim.Adam(G.parameters(), lr=self.args.gan_lr, betas=(0.5, 0.999))
        D_Opt = optim.Adam(D.parameters(), lr=self.args.gan_lr, betas=(0.5, 0.999))

        #Matrix of shape [10,10,1,1] with 1s at positions
        #where shape[0]==shape[1]
        GVec = torch.zeros(10, 10)
        GVec = GVec.scatter_(1, torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7,
                                 8, 9]).view(10,1), 1).view(10, 10, 1, 1)

        #Matrix of shape [10,10,32,32] with 32x32 matrix of 1s
        #where shape[0]==shape[1]
        DVec = torch.zeros([10, 10, 32, 32])
        for i in range(10):
            DVec[i, i, :, :] = 1

        print("Starting GAN Training")
        for epoch in range(int(self.args.gan_epochs)):
            self.updateLR(epoch, G_Opt, D_Opt)
            #Make vectors of ones and zeros of same shape as output by
            #Discriminator so that it can be used in BCELoss
            D_like_real = torch.ones(self.args.gan_batch_size)
            D_like_fake = torch.zeros(self.args.gan_batch_size)
            if self.args.cuda:
                D_like_real = Variable(D_like_real.cuda())
                D_like_fake = Variable(D_like_fake.cuda())

            #Iterate over examples that the classifier trainer just iterated on
            #TODO Also add examples that are generated by GAN for active classes
            #dataIteratorForGan contains examples only from current increment
            for image, label in dataIterator:
                #When less than gan_batch_size examples remain
                batch_size = image.shape[0]
                if batch_size != self.args.gan_batch_size:
                    D_like_real = torch.ones(batch_size)
                    D_like_fake = torch.zeros(batch_size)
                    if self.args.cuda:
                        D_like_real = Variable(D_like_real.cuda())
                        D_like_fake = Variable(D_like_fake.cuda())
                ##################################
                #Train Discriminator
                ##################################
                #Train with real image and labels
                D.zero_grad()

                #Shape [batch_size, 10, 32, 32]. Each entry at D_labels[0]
                #contains 32x32 matrix of 1s inside D_labels[label] index
                #and 32x32 matrix of 0s otherwise
                D_labels = DVec[label]
                if self.args.cuda:
                    image    = Variable(image.cuda())
                    D_labels = Variable(D_labels.cuda())

                #Discriminator output for real image and labels
                print(image.shape, D_labels.shape)
                D_output = D(image, D_labels).squeeze()
                #Maximize the probability of D_output to be all 1s
                print(D_output.shape, D_like_real.shape)
                D_real_loss = criterion(D_output, D_like_real)

                #Train with fake image and labels
                G_random_noise = torch.randn((batch_size, 100))
                G_random_noise = G_random_noise.view(-1, 100, 1, 1)

                #Generating random batch_size of labels from amongst
                #labels present in activeClass
                active_classes_tensor = torch.LongTensor(activeClasses)
                random_labels = (torch.rand(batch_size, 1)
                                 * len(active_classes_tensor)).type(
                                 torch.LongTensor).squeeze()
                random_labels = active_classes_tensor[random_labels]

                #Convert labels to appropriate shapes
                G_random_labels = GVec[random_labels]
                D_random_labels = DVec[random_labels]

                if self.args.cuda:
                    G_random_noise  = Variable(G_random_noise.cuda())
                    G_random_labels = Variable(G_random_labels.cuda())
                    D_random_labels = Variable(D_random_labels.cuda())

                G_output = G(G_random_noise, G_random_labels)
                D_output = D(G_output, D_random_labels).squeeze()

                D_fake_loss = criterion(D_output, D_like_fake)
                D_Loss = D_real_loss + D_fake_loss
                D_Loss.backward()
                D_Opt.step()

                #################################
                #Train Generator
                #################################
                G.zero_grad()
                #Follow same steps, but change the loss
                #TODO put this in a function instead
                G_random_noise = torch.randn((batch_size, 100))
                G_random_noise = G_random_noise.view(-1, 100, 1, 1)

                #Generating random batch_size of labels from amongst
                #labels present in activeClass
                active_classes_tensor = torch.LongTensor(activeClasses)
                random_labels = (torch.rand(batch_size, 1)
                                 * len(active_classes_tensor)).type(
                                 torch.LongTensor).squeeze()
                random_labels = active_classes_tensor[random_labels]

                #Convert labels to appropriate shapes
                G_random_labels = GVec[random_labels]
                D_random_labels = DVec[random_labels]

                if self.args.cuda:
                    G_random_noise  = Variable(G_random_noise.cuda())
                    G_random_labels = Variable(G_random_labels.cuda())
                    D_random_labels = Variable(D_random_labels.cuda())

                G_output = G(G_random_noise, G_random_labels)
                D_output = D(G_output, D_random_labels).squeeze()

                #We are maximizing the opposite of discriminator fake cost
                G_Loss = criterion(D_output, D_like_real)
                G_Loss.backward()
                G_Opt.step()
                
                print(D_Loss, G_Loss)

    #TODO Merge this with updateLR function in classifierTrainer
    #TODO Add the new args to runExperiment
    def updateLR(self, epoch, G_Opt, D_Opt):
        for temp in range(0, len(self.args.gan_schedule)):
            if self.args.gan_schedule[temp] == epoch:
                #Update Generator LR
                for param_group in G_Opt.param_groups:
                    currentLr_G = param_group['lr']
                    param_group['lr'] = currentLr_G * self.args.gan_gammas[temp]
                    print("Changing GAN Generator learning rate from",
                          currentLr_G, "to", currentLr_G * self.args.gan_gammas[temp])
                #Update Discriminator LR
                for param_group in D_Opt.param_groups:
                    currentLr_D = param_group['lr']
                    param_group['lr'] = currentLr_D * self.args.gan_gammas[temp]
                    print("Changing GAN Generator learning rate from",
                          currentLr_D, "to", currentLr_D * self.args.gan_gammas[temp])
